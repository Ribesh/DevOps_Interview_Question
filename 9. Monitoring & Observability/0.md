## Q1: Explain the three pillars of observability: metrics, logs, and traces
**Why ask:** Foundation for modern monitoring practices.

### Answer:
**Expected explanation:**

| Pillar | Purpose | Tools | Example |
|--------|---------|-------|---------|
| **Metrics** | Quantitative performance data | Prometheus, Grafana, DataDog | CPU: 45%, Memory: 60%, Requests/sec: 100 |
| **Logs** | Detailed event information | ELK Stack, Splunk, Loki | "Database connection failed at 14:32:01" |
| **Traces** | Request flow across services | Jaeger, Datadog, X-Ray | Request → API → DB (total: 250ms) |

**Cohesion:** "When a user reports slow performance, I correlate metrics (high CPU), logs (database error), and traces (DB query took 5s) to identify the root cause"


## Q2: Design a monitoring strategy for a microservices architecture
**Why ask:** Practical monitoring at scale.

### Answer:
**Expected components:**
1. **Metrics collection:**
   - Application metrics (HTTP Status Code [200], response time, request count, errors)
   - Infrastructure metrics (CPU, memory, disk, network)

2. **Aggregation:** Prometheus scraping, centralized collection
3. **Visualization:** Grafana dashboards per service/team
4. **Alerting:**
   - Alert thresholds based on SLOs
   - Alert routing (critical → on-call, warning → team)
   - Escalation policies
5. **Logging:** Centralized (ELK, Splunk) with structured logging
6. **Tracing:** End-to-end request tracing (Jaeger, X-Ray)
7. **Incidents:** On-call rotation, runbooks, post-mortems

**Example architecture:** "Each microservice exposes Prometheus metrics → Central Prometheus scrapes → Grafana visualizes → AlertManager routes alerts → PagerDuty pages engineer"


## Q3: A service is experiencing degraded performance. Walk through your monitoring-based debugging
**Scenario:** Real-world troubleshooting.

### Answer:
**Expected approach:**
1. **Check metrics:** 
   - Response latency: Is it increased?
   - Error rate: Are errors spiking?
   - Resource utilization: CPU, memory, disk usage?
   - Database query time: Is DB slow?
2. **Correlate with changes:**
   - Was code deployed recently?
   - Did infrastructure change?
   - Is database under load?
3. **Drill down with logs:**
   - Query logs for errors during degradation window
   - Look for specific error patterns
   - Correlate with metric changes
4. **Use traces:**
   - Identify which service in the chain is slow
   - Find expensive queries
5. **Implement fix:**
   - Scale horizontally if CPU-bound
   - Optimize slow queries if database-bound
   - Restart if memory leak suspected

**Example:** "Latency spike correlated with deployment. Traces showed new code path spending 5s in external API call. Rollback immediately while investigating, then re-deployed with timeout/circuit breaker"


## Q4: How do you set up Prometheus alerting? Describe alert design best practices
**Why ask:** Alerting is critical; tests understanding of alert fatigue and relevance.
### Answer:
- **Prometheus + Alertmanager architecture:**
  - Prometheus evaluates alert rules
  - Alertmanager handles routing, grouping, silencing
  - Notification channels: PagerDuty, Slack, email
  
- **Alert design best practices:**
  - **Actionable:** Alert only on symptoms, not causes
  - **Severity levels:** Critical (page on-call), Warning (notify team), Info (log)
  - **Thresholds based on SLOs:** Not arbitrary percentages
  - **Reduce alert fatigue:** Group related alerts, intelligent routing
  - **Example bad alert:** CPU > 50% (too noisy on large machines)
  - **Example good alert:** P99 latency > SLO for 5 minutes (actionable, threshold-based)

- **Rules example:**
  ```
  - alert: HighErrorRate
    expr: rate(http_requests_total{status=~"5.."}[5m]) > 0.05
    for: 5m
    annotations:
      summary: "High error rate detected"
  ```


## Q5: Compare Prometheus + Grafana vs ELK Stack for monitoring
**Why ask:** Tool selection based on use case.

### Answer:
**Expected comparison:**

| Aspect | Prometheus + Grafana | ELK Stack |
|--------|----------------------|-----------|
| **Best for** | Metrics, alerting, time-series | Logs, full-text search, analysis |
| **Data model** | Metrics (time-series) | Logs (unstructured text) |
| **Query language** | PromQL (powerful) | Kibana DSL / Lucene |
| **Scalability** | Excellent for metrics | Massive log volumes possible |
| **Cost** | Lower (simpler data) | Higher (storage-intensive) |
| **Setup complexity** | Easier | More components, complex |
| **Real-time** | Excellent | Good but slower |
| **Use cases** | Performance, availability | Debugging, compliance, auditing |

**Answer:** "For our infrastructure, I use Prometheus+Grafana for operational metrics and alerting, and ELK for application logs and post-incident investigation. Together, they provide complete observability"

## Q31: You have 10,000 alert rules in Prometheus. They're generating thousands of false positives. How do you fix this?
**Scenario:** Alert management at scale.

### Answer:
**Expected approach:**
1. **Analyze alert patterns:**
   - Which alerts trigger most often?
   - Which alerts actually indicate real problems?
   - Use recording rules to aggregate alerts
2. **Reduce noise:**
   - Adjust thresholds based on actual service behavior
   - Extend evaluation duration (`:for: 5m` instead of immediate)
   - Use multi-condition rules to reduce spurious alerts
3. **Restructure alerts:**
   - Focus on symptoms, not causes
   - Group related alerts
   - Add context in annotations
4. **Implement smart routing:**
   - Route high-confidence alerts to PagerDuty
   - Route low-confidence alerts to team Slack
   - Implement escalation policies
5. **Monitoring the monitors:**
   - Alert notification delivery SLA
   - Track alert acknowledgment time
   - Refine based on incident follow-ups
